# Proyecto CUDA - DQN y Ejemplos

Implementaci√≥n de Deep Q-Network (DQN) con CUDA para entrenamiento de agentes de reinforcement learning.

## üéØ Objetivo del Proyecto

El agente DQN se entrena para **resolver el problema CartPole**: equilibrar un p√©ndulo invertido sobre un carrito m√≥vil.

### El Desaf√≠o
- **Problema**: Un carrito con un p√©ndulo encima. El agente debe mover el carrito (izquierda/derecha) para mantener el p√©ndulo vertical.
- **Estado**: 4 dimensiones (posici√≥n carrito, velocidad, √°ngulo p√©ndulo, velocidad angular)
- **Acciones**: 2 opciones (mover izquierda o derecha)
- **Recompensa**: +1 por cada paso que el p√©ndulo se mantiene balanceado

### Criterios de √âxito
- **Excelente**: Promedio ‚â• 450 puntos en test (p√©ndulo balanceado casi 500 pasos)
- **Buen progreso**: Promedio ‚â• 200 puntos
- **Necesita m√°s entrenamiento**: < 200 puntos

El episodio termina cuando:
1. El carrito se sale del √°rea (`|x| > 2.4`)
2. El p√©ndulo se inclina mucho (`|theta| > 12¬∞`)
3. Se alcanzan 500 pasos (m√°ximo)

## Estructura del Proyecto

### üìÅ `include/` - Headers

#### `dqn.cuh`
- **Clase `DQN`**: Implementaci√≥n completa del algoritmo Deep Q-Network
  - Policy network (Q-network) y Target network
  - Epsilon-greedy para exploraci√≥n
  - Replay buffer para almacenar experiencias
  - M√©todos: `select_action()`, `train_step()`, `update_target_network()`

- **Clase `CartPoleEnv`**: Ambiente de simulaci√≥n CartPole
  - 4 estados: posici√≥n, velocidad, √°ngulo, velocidad angular
  - 2 acciones: mover izquierda o derecha
  - M√©todos: `reset()`, `step()`

#### `neural_network.cuh`
- **Clase `NeuralNetwork`**: Red neuronal fully-connected en CUDA
  - Forward propagation en GPU
  - Backward propagation con gradientes
  - Activaci√≥n ReLU
  - Actualizaci√≥n de pesos con SGD
  - M√©todos: `forward()`, `backward()`, `copy_weights_from()`

- **Kernels CUDA**: Operaciones de red neuronal paralelizadas
  - `fully_connected_forward`: Multiplicaci√≥n matriz-vector
  - `relu_activation`: Funci√≥n de activaci√≥n
  - `relu_backward`: Gradiente de ReLU
  - `fully_connected_backward`: Backpropagation
  - `update_weights`: Actualizaci√≥n de par√°metros

#### `replay_buffer.cuh`
- **Clase `ReplayBuffer`**: Buffer circular para experiencias
  - Almacena transiciones (state, action, reward, next_state, done)
  - Datos en GPU para acceso r√°pido
  - Muestreo aleatorio de batches
  - M√©todos: `add()`, `sample()`, `can_sample()`

### üìÅ `src/` - Implementaciones

#### `main_dqn.cu` (Programa principal DQN)
- Loop de entrenamiento completo de 500 episodios
- Configuraci√≥n: arquitectura 4‚Üí64‚Üí64‚Üí2
- M√©tricas de progreso (reward promedio, epsilon, pasos)
- Fase de test sin exploraci√≥n (10 episodios)
- Objetivo: Lograr 450+ puntos promedio (p√©ndulo balanceado 90% del tiempo)
- Ejecutable: `dqn_train`

#### `dqn.cu`
Implementaci√≥n de:
- Algoritmo DQN completo (Deep Q-Learning)
- Epsilon-greedy strategy (exploraci√≥n vs explotaci√≥n)
- Training step con batch sampling del replay buffer
- Target network update cada N episodios
- CartPole physics simulation (ecuaciones de movimiento del p√©ndulo)
- Funci√≥n Q(estado, acci√≥n) para predecir recompensas futuras

#### `neural_network.cu`
Implementaci√≥n de:
- Inicializaci√≥n de capas con Xavier/He
- Forward pass paralelo
- Backward pass con chain rule
- Copia de pesos entre redes (policy ‚Üí target)

#### `replay_buffer.cu`
Implementaci√≥n de:
- Buffer circular eficiente
- Gesti√≥n de memoria GPU
- Muestreo aleatorio uniforme
- Almacenamiento de experiencias

## Compilaci√≥n

```bash
# Windows
compile_dqn.bat

# Linux/Mac
./compile_dqn.sh
```

## Ejecutables Generados

- **`cuda_hello`**: Programa de prueba b√°sico de CUDA
- **`dqn_train`**: Entrenamiento de agente DQN en CartPole

## Dependencias

- CUDA Toolkit 12.8
- Compute Capability 5.0+ (MX110 compatible)
- CMake 3.18+
- cuRAND (generaci√≥n aleatoria en GPU)

## Arquitectura DQN

```
Estado (4) ‚Üí FC(64) ‚Üí ReLU ‚Üí FC(64) ‚Üí ReLU ‚Üí FC(2) ‚Üí Q-values
```

- Learning rate: 0.001
- Gamma: 0.99
- Epsilon decay: 0.995
- Batch size: 32
- Replay buffer: 10,000 experiencias
